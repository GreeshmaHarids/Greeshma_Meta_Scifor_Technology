{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMKFev72gA0C5hvTH2b5BGU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GreeshmaHarids/Greeshma_Meta_Scifor_Technology/blob/main/Machine_Learning/Week_Assessments/Test_5-NLP_Statistics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NLP\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Y8o-vihXwSob"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Write a program of text processing\n",
        "\n",
        "Steps to text processing:\n",
        "\n",
        "\n",
        "*   Convert all the text to lowercase\n",
        "*   Split text into individual words/tokens\n",
        "*   Removing special characters\n",
        "*   Removing stop words and punctuations\n",
        "*   Stemming or Lemmatization\n",
        "\n"
      ],
      "metadata": {
        "id": "t5_xQa4MwZvf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "7Z1psNSnuz5I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc6c51ac-118e-4e19-e466-528d5ac1b0b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Hello!... Everyone today is November 26th and its 3pm and I am going to read.\"\n",
        "\n",
        "#creating function to do text processing\n",
        "def text_process(text):\n",
        "\n",
        "  #lowercase\n",
        "  text = text.lower()\n",
        "  print(\"After Lowercase:\\n\", text)\n",
        "\n",
        "  #tokenization\n",
        "  tokens = word_tokenize(text)\n",
        "  print(\"\\nTokens:\\n\", tokens)\n",
        "\n",
        "  #Removing special characters\n",
        "  tokens = [word for word in tokens if word.isalpha()]  # Keep only alphabetic words\n",
        "  print(\"\\nAfter Removing Special Characters:\\n\", tokens)\n",
        "\n",
        "  #Removing stop words\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  tokens = [word for word in tokens if word not in stop_words]\n",
        "  print(\"\\nAfter Removing Stop Words:\\n\", tokens)\n",
        "\n",
        "  #stemming\n",
        "  stemmer = PorterStemmer()\n",
        "  tokens = [stemmer.stem(word) for word in tokens]\n",
        "  print(\"\\nAfter Stemming:\\n\", tokens)\n",
        "\n",
        "  #lemmatization\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "  print(\"\\nAfter Lemmatization:\\n\", tokens)\n",
        "\n",
        "\n",
        "text_process(text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mH1hjmOqy9Nf",
        "outputId": "d1031254-ec83-423f-8393-3bb00bfa3495"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After Lowercase:\n",
            " hello!... everyone today is november 26th and its 3pm and i am going to read.\n",
            "\n",
            "Tokens:\n",
            " ['hello', '!', '...', 'everyone', 'today', 'is', 'november', '26th', 'and', 'its', '3pm', 'and', 'i', 'am', 'going', 'to', 'read', '.']\n",
            "\n",
            "After Removing Special Characters:\n",
            " ['hello', 'everyone', 'today', 'is', 'november', 'and', 'its', 'and', 'i', 'am', 'going', 'to', 'read']\n",
            "\n",
            "After Removing Stop Words:\n",
            " ['hello', 'everyone', 'today', 'november', 'going', 'read']\n",
            "\n",
            "After Stemming:\n",
            " ['hello', 'everyon', 'today', 'novemb', 'go', 'read']\n",
            "\n",
            "After Lemmatization:\n",
            " ['hello', 'everyon', 'today', 'novemb', 'go', 'read']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Write a Program to Implement NLP Based on SpaCy:\n"
      ],
      "metadata": {
        "id": "nXEdUzTn0GIz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the English language model\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n"
      ],
      "metadata": {
        "id": "wBknpEmU0v2Q"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text=nlp(\n",
        "    \"Hello!... Everyone today is November 26th and its 3pm and I am going to read.\"\n",
        ")\n",
        "\n",
        "print(type(text))\n",
        "\n",
        "[token.text for token in text]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5CUwmtIZ1GAQ",
        "outputId": "eca573b7-0884-43e5-9cec-453232908b70"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'spacy.tokens.doc.Doc'>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " '!',\n",
              " '...',\n",
              " 'Everyone',\n",
              " 'today',\n",
              " 'is',\n",
              " 'November',\n",
              " '26th',\n",
              " 'and',\n",
              " 'its',\n",
              " '3',\n",
              " 'pm',\n",
              " 'and',\n",
              " 'I',\n",
              " 'am',\n",
              " 'going',\n",
              " 'to',\n",
              " 'read',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# for sentence detection\n",
        "sent=\"\"\"Hello... all how are you.\n",
        "Today is Friday.Happy to be here\"\"\"\n",
        "\n",
        "sent_doc=nlp(sent)\n",
        "sentence = list(sent_doc.sents)\n",
        "\n",
        "for i in sentence:\n",
        "  print(f\"'{i[:5]}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBbJeai61rkg",
        "outputId": "312d70b6-41aa-410d-845d-6b3d17f2e06d"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Hello... all how are'\n",
            "'Today is Friday.'\n",
            "'Happy to be here'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #Tokenization: Extract tokens\n",
        "print(\"Tokens:\")\n",
        "for token in sent_doc:\n",
        "    print(token.text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPFX_9au5GTM",
        "outputId": "c9138d4f-4d80-4ee4-ee13-38b99ad92218"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens:\n",
            "Hello\n",
            "...\n",
            "all\n",
            "how\n",
            "are\n",
            "you\n",
            ".\n",
            "\n",
            "\n",
            "Today\n",
            "is\n",
            "Friday\n",
            ".\n",
            "Happy\n",
            "to\n",
            "be\n",
            "here\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove special characters (non-alphabetic)\n",
        "tokens_clean = [token.text for token in sent_doc if token.is_alpha]\n",
        "\n",
        "print(\"Cleaned Tokens (No special characters): \", tokens_clean)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rUDn500p6c1P",
        "outputId": "234b3595-7c6f-41ca-85ee-9a3628fd5229"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned Tokens (No special characters):  ['Hello', 'all', 'how', 'are', 'you', 'Today', 'is', 'Friday', 'Happy', 'to', 'be', 'here']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Remove stop words and punctuation\n",
        "tokens_filtered = [token.text for token in sent_doc if not token.is_stop and not token.is_punct]\n",
        "\n",
        "print(\"Filtered Tokens (No stop words & punctuation): \", tokens_filtered)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RPA7aUKP65-J",
        "outputId": "f6c3b317-def5-4644-9106-52feb7bd9bce"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered Tokens (No stop words & punctuation):  ['Hello', '\\n', 'Today', 'Friday', 'Happy']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10: Lemmatization (getting the base form of the words)\n",
        "tokens_lemmatized = [token.lemma_ for token in sent_doc if not token.is_stop and not token.is_punct]\n",
        "\n",
        "print(\"Lemmatized Tokens: \", tokens_lemmatized)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPm84sMW7M-X",
        "outputId": "316823c3-a37d-41c1-8079-fc45d6e1914b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmatized Tokens:  ['hello', '\\n', 'today', 'Friday', 'happy']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Part-of-Speech Tagging\n",
        "print(\"\\nPart-of-Speech Tags:\")\n",
        "for token in sent_doc:\n",
        "    print(f\"{token.text}: {token.pos_}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUtuuero5fko",
        "outputId": "6bd7e83f-b117-462c-ce0e-e888806f7b71"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Part-of-Speech Tags:\n",
            "Hello: INTJ\n",
            "...: PUNCT\n",
            "all: PRON\n",
            "how: SCONJ\n",
            "are: AUX\n",
            "you: PRON\n",
            ".: PUNCT\n",
            "\n",
            ": SPACE\n",
            "Today: NOUN\n",
            "is: AUX\n",
            "Friday: PROPN\n",
            ".: PUNCT\n",
            "Happy: ADJ\n",
            "to: PART\n",
            "be: AUX\n",
            "here: ADV\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Named Entity Recognition (NER)\n",
        "print(\"\\nNamed Entities:\")\n",
        "for ent in sent_doc.ents:\n",
        "    print(f\"{ent.text} ({ent.label_})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCqf-aza53Ay",
        "outputId": "75f2d935-e568-410f-8230-745d8d7c94ea"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Named Entities:\n",
            "Today (DATE)\n",
            "Friday (DATE)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Statistics"
      ],
      "metadata": {
        "id": "9vhzCVLe8ble"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.  Difference between descriptive and inferential statistics."
      ],
      "metadata": {
        "id": "VhT9znZF8Y6u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Descriptive statistics summarize or describes data\n",
        "  where as inferential statistics makes inferences or conclusions about the population based on the sample.\n",
        "\n",
        "2. Types of descriptive statistics are:\n",
        "Measure of Central Tendency: Mean, Median, and Mode.\n",
        "Measure of Variability: Range, Variance, and Standard Deviation.\n",
        "Measure of frequency: frequency table, contingency table.\n",
        "Graphical representations like pie charts, bar charts, etc.\n",
        "\n",
        "  for inferential statistics: Tests of significance such as hypothesis testing, regression analysis, ANOVA, chi-square, etc.\n",
        "\n",
        "3. In descriptive statistics use entire dataset and for inferencial statistics uses a sample of the dataset.\n",
        "\n",
        "4. for descriptive :No assumption is needed about the underlying population.\n",
        "for inferential we make some assumptions about the population.\n",
        "5. descriptive statistics is mainly used for : organize, analyze, and present the data in a meaningful way.\n",
        "where as inferential uses for compare, test, and predict the data.\n",
        "6.  descriptive stat is not dependent on probability. But inferential is strongly dependent on probability concepts.\n",
        "7. descriptive statistics is concerned with present or historical data.\n",
        "where as inferential is concerned with future or unseen data.\n",
        "8. descriptive statistics Limited to the data at hand. It does not allow for generalizations beyond the data.\n",
        "inferential statistics: allows generalization to a larger population based on sample data.\n",
        "9. descriptive statistics: less complex since it simply describes the dataset.\n",
        "inferential statistics: More complex, as it involves testing hypotheses, estimating parameters, and making predictions with a margin of error.\n",
        "10. descriptive statistics: provides exact and precise information about the dataset without any error margin.\n",
        "inferential statistics provides estimates and predictions.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Tyyl0wda8iwn"
      }
    }
  ]
}